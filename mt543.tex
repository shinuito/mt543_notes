\documentclass[12pt,a4paper]{article}
%packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{physics}
\usepackage{tikz-cd}
\usepackage{mathtools} % for text over and under iff etc, underset, underbrace etc

\usepackage{chngcntr}
\counterwithout{subsection}{section}
\setcounter{subsection}{-1}

\title{MT543 Topics in Algebra}
\author{Notes taken by Stephen Nulty and John Brennan}
\renewcommand{\abstractname}{Note:} 

%new commands
\newcommand{\rR}{\ensuremath{\mathbb{R}\,}}
\newcommand{\cC}{\ensuremath{\mathbb{C}\,}}
\newcommand{\hH}{\ensuremath{\mathbb{H}\,}}
\newcommand{\fF}{\ensuremath{\mathbb{F}\,}}
\newcommand{\qQ}{\ensuremath{a+bi+cj+dk\,}}
\newcommand{\nn}{\ensuremath{n \times n\,}}

\newcommand{\mnr}{\ensuremath{M_n(\rR)\,}}
\newcommand{\mnc}{\ensuremath{M_n(\cC)\,}}
\newcommand{\mnh}{\ensuremath{M_n(\hH)\,}}
\newcommand{\mr}[1]{\ensuremath{M_{#1}(\rR)\,}}
\newcommand{\mc}[1]{\ensuremath{M_{#1}(\cC)\,}}
\newcommand{\mh}[1]{\ensuremath{M_{#1}(\hH)\,}}
\newcommand{\glnr}{\ensuremath{GL_n(\rR)\,}}
\newcommand{\glnc}{\ensuremath{GL_n(\cC)\,}}
\newcommand{\glnh}{\ensuremath{GL_n(\hH)\,}}
\newcommand{\glr}[1]{\ensuremath{GL_{#1}(\rR)\,}}
\newcommand{\glc}[1]{\ensuremath{GL_{#1}(\cC)\,}}
\newcommand{\glh}[1]{\ensuremath{GL_{#1}(\hH)\,}}

\newcommand{\ra}{\ensuremath{\Rightarrow}}
\newcommand{\la}{\ensuremath{\Leftarrow}}
\newcommand{\cin}{\ensuremath{\mathcal{I}_n\,}}
\newcommand{\ci}[1]{\ensuremath{\mathcal{I}_{#1}\,}}
\newcommand{\im}{\ensuremath{\operatorname{im}}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\ipm}[2]{\ensuremath{\left\langle #1, \, #2 \right\rangle}}

%proofs etc
\newtheorem{thm}{Theorem}[subsubsection]
\newtheorem{defn}[thm]{Definition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{obs}[thm]{Observation}
\newtheorem{cor}[thm]{Corollary}

%block matrices
\newcommand{\rvline}{\hspace*{-\arraycolsep}\vline\hspace*{-\arraycolsep}}
\newcommand{\bigzero}{\mbox{\normalfont\Large\bfseries 0}}
\newcommand{\bigld}{\mbox{\normalfont\Large\bfseries \ldots}}
\newcommand{\bigvd}{\mbox{\normalfont\Large\bfseries \vdots}}

\renewcommand{\thesubsection}{\arabic{subsection}}
\makeatletter
\def\@seccntformat#1{\@ifundefined{#1@cntformat}%
   {\csname the#1\endcsname\quad}%    default
   {\csname #1@cntformat\endcsname}}% enable individual control
\newcommand\section@cntformat{}     % section level 
\newcommand\subsection@cntformat{\thesubsection.\space} % subsection level
\newcommand\subsubsection@cntformat{\thesubsubsection.\space} % subsubsection level
\makeatother

\renewcommand{\thethm}{\arabic{subsection}.\arabic{subsubsection}.\arabic{thm}}

%begin
\begin{document}
\maketitle
\begin{abstract}
    Any transcription mistakes and typos are my own.
\end{abstract}
Lectures by David Wraith. Lie Groups and Lie Algebras.
\section{Lecture 1 25/09/23}
missed this lecture - some intro to do with spheres, transformations and symmetries and other motivational stuff. Definition of an algebra (bilinear product) over a field.

\subsection{Introduction}

Lie groups have a dual nature: they are groups but also very special topological spaces. The algebraic and topological (spatial) properties are closely aligned. Lie groups and Lie algebras lie at the intersection of algebra, topology, geometry, analysis and more.

\begin{defn}
An algebra is a vector space $V$ equipped with a bilinear map $m:V\times V \to V$.
\end{defn}

Note, the ``multiplication" map $m$ does not have to be commutative or associative. In general, Lie algebras are neither commutative nor associative. Recall,
\begin{align*}
&\mbox{Commutativity: } m(u, v) = m(v, u), \\
&\mbox{Associativity: } m(m(u, v), w) = m(u, m(v, w)).
\end{align*}
Every Lie group has an associated Lie algebra which encodes many properties of the group. Often this allows problems about Lie groups to be reduced to problems in (fancy!) linear algebra.

\subsection*{Example of a Lie group}

The set of rotations of a ball centred on $O$ in $\rR^3$ is a Lie group. It is a group under composition of rotations. To ``see" the topology here, notice that it makes sense to talk about two rotations being ``close", so there is a sense of space. It makes sense to consider a continuous family of rotations. Continuity implies the existence of topology. We can identify this group with the matrix group $SO(3)$. The map $\rR^3 \to \rR^3$ given by $x \mapsto Ax$ for $A \in SO(3)$ is a rotation and every rotation occurs in this way. $SO(3)$ is a subset (but not a subgroup) of the set/group of all $(3\times 3)$-real matrices $M_3(\rR)$. By listing the elements of any $3\times 3$ matrix we get a bijection $M_3(\rR) \to \rR^9$. As $\rR^9$ has a natural topology (metric), this gives a natural topology on $M_3(\rR)$ and by restriction on $SO(3)$.


\section{Lecture 2 27/09/23}
Sorting out tutorial times. Lectures: Monday 2pm MS2, Wednesday 2pm LGH, Thursday 12pm MS2.

Lie Groups, dual nature, Groups but also a topological geometrical character. Can prove things with a mix of both methods - intersection of various areas.

\subsection{Groups of matrices}
\subsubsection{General Linear Groups}

Quaternions will have a central role.

Consider groups of $N\times N$ matrices over the fields \rR and \cC and also over the quaternions.

\begin{defn}
The quaternions \hH is a 4-dim real vector space with standard basis elements $1,i,j,k$, equipped with an associative linear multiplication operation defined by

\[i^2=j^2=k^2=-1, \quad ij=k, jk=i, ki=j\]
\end{defn}

So a generic quaternion takes the form $a+bi+cj+dk$, $a,b,c,d \in \rR$.

Observe, $ji=j(jk)=(jj)k$ (by associativity) $=j^2k=-k$. Similarly $kj=-i$ and $ik=-j$.

e.g. $(2+i-3k)(5+2i-j+k)=10+4i-2j+2k+5i-2-k-j-15k-6j-61+3$ etc.

Quaternions is not commutative, so is not a field. However it is a skew field (division algebra).

Terminology - In $a+bi+cj+dk$, $a$ is called the real or scalar part, and the rest $bi+cj+dk$ imaginary or vector part.

In analogy with complex numbers,

\begin{defn} 
1. The conjugate of  \qQ , is $\overline{\qQ}=a-bi-cj-dk$
2. The norm of \qQ is  $|\qQ |=\sqrt{a^2+b^2+c^2+d^2}$
\end{defn}

Thus \hH is a normed vector space. Next observe that for each $q\in\hH$ $q\bar{q}=\bar{q}q = |q|^2$.

therefore (symbol) $q^{-1}=\bar{q}/|q|^2$. So $qq^{-1}=q\bar{q}/|q|^2=|q|^2=1$, similarly for $q^{-1}q=1$.

This allows division $q_1\cdot q_2^{-1}=q_1 \bar{q_2}/|q_2|^2$. Writing $q_1/q_2$ is ambiguous however. $q_1 q_2^{-1}\neq q_2^{-1} q_1$ generically.

Clearly $\rR \subset \cC \subset \hH$. A classic theorem of Frobenius asserts that \rR, \cC, \hH are the only real associative division algebras. These objects similarly play a distinguished role in Lie group theory.

Convention: Suppose V is a vector space over the quaternions \hH. We will adopt the convention that whenever we scale a vector $v\in V$ by a scalar $\lambda \in \hH$, we multiply on the left, i.e. $\lambda v$

Let $M_n(\rR),  M_n(\cC),  M_n(\hH)$ denote the sets (vector spaces!) of all $n\times n$ matrices over $\rR, \cC,  \hH$.

\begin{defn} 
The General Linear Groups $GL_n(\rR)$, resp. $GL_n(\cC)$ is the group of  $n\times n$ invertible matrices with \rR resp \cC coefficients. (Group under multiplication). Equivalently $GL_n(\rR)=\{A\in M_n(\rR)| \det(A)\neq0\}$. Similarly $GL_n(\cC)=\{A\in M_n(\cC)| \det(A)\neq0\}$.
\end{defn}

(return to the idea of determinants of quaternions later).

Recall that for any matrix $A\in\mnr$ we have two associated linear maps $L_A:\rR^n\to \rR^n, L_a(\vec{x})=A\vec{x},$  $R_A:\rR^n\to \rR^n, R_a(\vec{x})=\vec{x}A.$ 

It is well know that $A$ is invertible (RC cases) $\iff$ $\det(A)\neq0$ $\iff$ $L_A, R_A$ are isomorphisms.

\section{Lecture 3 02/10/23}
Thursday lecture moved to Friday at 10am in MS2.

Reminder: 
\begin{itemize}
\item Quaternions \hH, multiplication is associative not commutative. If $V$ is a \hH - vector space, we scale from the left only, i.e. $\lambda v$ for $\lambda \in \hH, v\in V$. 
\item General linear groups \glnr, \glnc groups under $*$ of all invertible \rR resp. \cC \nn - matrices.
\item $A\in \mnr, \mnc$ is invertible iff $\det A \neq 0$ iff $L_A, R_A$ are both invertible where $L_a(\vec{x})=A\vec{x}$, $R_a(\vec{x})=\vec{x}A.$ 
\end{itemize}

We now consider \mnh.

\begin{defn}
A function $f:\hH ^n \to \hH^n$ is \hH - linear if  $f ( \lambda _1 v_1+\lambda _2 v_2)=\lambda _1 f(v_1)+\lambda _2 f(v_2),\; \forall \lambda _1 \lambda _2 \in \hH , v_1,v_2 \in \hH ^n$.
\end{defn}

\begin{lemma}
For $A \in \mnh$, $R_A:\hH^n\to \hH^n$ given by $R_a(\vec{x})=\vec{x}A$  for $v\in \hH^n$ a row vector, is \hH - linear, however $L_A$ is in general not \hH - linear.
Proof: exercise
\end{lemma}

idea is that associativity makes $\lambda v A$ ok, but not with left multiplication which is interfered by commutativity.

\begin{lemma}
For $A \in \mnh$, $R_A:\hH^n\to \hH^n$, is an \hH - linear isomorphism iff $A$ is invertible, i.e. $\exists B\in \mnh$ such that $AB=BA=I_n$.
\end{lemma}

\begin{proof}
($\ra$) If $R_A$ is an iso. then there is a \hH -linear inverse $(R_A)^{-1}: \hH ^n \to \hH ^n $. There is a corresponding matrix $B\in \mnh$. Since $R_A \circ (R_A)^{-1}=R_A \circ (R_A)^{-1}=I_n$. we deduce $BA=AB=I_n$ (NB order of matrices here!). Therefore $B=A^{-1}$.

($\la$) Similar.
\end{proof}

\begin{defn}
The quaternionic general linear group $\glnh = \{A\in \mnh | A \text{ is invertible}\} = \{A\in \mnh| R_a \text{ is an iso.}\}$
\end{defn}

NB: There is a problem with the notion of \hH - determinant due to non-commutativity we'll return to this later (possible to define determinant and \glnh as ones with non-zero determinant, but defining it requires some thought.)

It turns out that we can view \cC and \hH- matrices/linear maps in terms of \rR- matrices. 

\begin{prop}
There is a real linear map $\rho_n: \mnc \to \mr{2n}$ such that the following diagram commutes. 
\begin{center}
\begin{tikzcd}
  \cC^n \arrow{d}{R_A} \arrow{r}{\theta_n}
    & \rR^{2n} \arrow{d}{R_{\rho_n(A)}} \\
  \cC^n \arrow{r}{\theta_n}
&\rR^{2n} \end{tikzcd}
\end{center}
where $\theta _n : \cC ^n \to \rR ^{2n}$ is given by $\theta _n (a_1+ib_1, \ldots , a_n+ib_n )=(a_1,b_1,\ldots, a_n,b_n)$.
\end{prop}

(compactly every complex matrix can be viewed as a real matrix of twice the size)

Remark: $\theta _n $ is  a real linear isomorphism. This forces $R_{\rho _n (A)}=\theta _n \circ R_A \circ \theta _n^{-1}$.

This is linear and therefore there is a corresponding matrix $\in \mr{2n}$.

\begin{proof}
See moodle.
\end{proof}

\begin{obs}
$\rho_n$ is injective. Proof: exercise.
\end{obs}

\begin{lemma}
$\rho _n$ satisfies $\rho_n(AB)= \rho_n(A)\rho_n(B)$. So $\rho_n$ is  an injective real-algebra homomorphism.
\end{lemma}
\begin{proof}
We compose commutative squares from 1.1.8 to get

\begin{center}
\begin{tikzcd}
  \cC^n \arrow{d}{R_A} \arrow{r}{\theta_n}
    & \rR^{2n} \arrow{d}{R_{\rho_n(A)}} \\
  \cC^n \arrow{r}{\theta_n}  \arrow{d}{R_B}
&\rR^{2n} \arrow{d}{R_{\rho_n(B)}} \\
  \cC^n \arrow{r}{\theta_n}
&\rR^{2n} 

\end{tikzcd}
\end{center}

On L.H.S. we have $R_B \circ R_A = R_{AB}$. (note order)

On R.H.S we have $R_{\rho_n(B)} \circ R_{\rho_n(A)} =R_{\rho_n(A)\rho_n(B)}$.

But since LHS is $R_{AB}$ this means $R_{\rho_n(AB)}= \text{composition on RHS}=R_{\rho_n(A)\rho_n(B)}$.
\end{proof}

It's not surjective however. Q: What exactly is $\rho_n(A)$? Consider $(a+ib)\in \mc{1}$. 

\[R_{(a+ib)}(x+iy)=(x+iy)(a+ib)=(ax-by)+i(ay+bx)\]

Now $\theta_1(x+iy)=(x,y) \in \rR^2$ etc.

So $\theta_1((ax-by)+i(ay+bx))=(ax-by,ay+bx)$

The corresponding map from $\rR ^2\to \rR^2$ is $(x,y)\mapsto (ax-by,ay+bx)$. Observe that 

\[\begin{pmatrix} x & y\end{pmatrix}\begin{pmatrix} a & b \\ -b & a \end{pmatrix}=(ax-by,ay+bx)\]

So $\begin{pmatrix} a & b \\ -b & a \end{pmatrix}\in \mr{2}$ corresponds under $\rho_1$ to $(a+ib)\in \mc{1}$.

More generally 

\[\begin{pmatrix}
  a_{11}+ib_{11} & \ldots &   a_{1n}+ib_{1n}\\
  \vdots & \vdots & \vdots \\
   a_{n1}+ib_{n1} & \ldots &   a_{nn}+ib_{nn}\\
\end{pmatrix} \in \mnc\]

corresponds to

\[
\begin{pmatrix}
  \begin{matrix}
  a_{11} & b_{11}  \\
  -b_{11} &   a_{11}
  \end{matrix}
  & \rvline & \bigld  & \rvline &
  \begin{matrix}
  a_{1n} & b_{1n}  \\
  -b_{1n} &   a_{1n}
  \end{matrix}  
  \\
\hline

  \bigvd
  & \rvline & \bigvd  & \rvline &
  \bigvd
  \\
\hline
    \begin{matrix}
  a_{n1} & b_{n1}  \\
  -b_{n1} &   a_{n1}
  \end{matrix}
  & \rvline & \bigld  & \rvline &
  \begin{matrix}
  a_{nn} & b_{nn}  \\
  -b_{nn} &   a_{nn}
  \end{matrix}  
\end{pmatrix} \in \mr{2n}
\]

 is obtained by replacing each \cC entry by its corresponding $2\times 2$ real block.

\section{Lecture 4 04/10/23}
Last time:

\begin{itemize}
\item $A\in \mnh$ then $R_A:\hH^n\to\hH^n$ given by $R_a(\vec{x})=\vec{x}A.$  is \hH - linear (assuming coefficients in \hH multiply on vectors from the left, x row vector). Left multiplication is not in general \hH linear.
\item Under the real linear isomorphism $\theta_n: \cC^n \to \rR^{2n},$ $\theta _n (a_1+ib_1, \ldots , a_n+ib_n )=(a_1,b_1,\ldots, a_n,b_n)$. Any complex-linear map $\cC^n\to \cC^n$ corresponds to a real-linear map $\rR^{2n}\to \rR^{2n}$ and in terms of matrices (an right multiplication) $A\in \mnc$ corresponds to some matrix $\rho_n(A)\in \mr{2n}$.
\item \[\text{If } A=\begin{pmatrix}
  a_{11}+ib_{11} & \ldots &   a_{1n}+ib_{1n}\\
  \vdots & \vdots & \vdots \\
   a_{n1}+ib_{n1} & \ldots &   a_{nn}+ib_{nn}\\
\end{pmatrix}\]
then 
\[
\rho_n(A)=\begin{pmatrix}
  \begin{matrix}
  a_{11} & b_{11}  \\
  -b_{11} &   a_{11}
  \end{matrix}
  & \rvline & \bigld  & \rvline &
  \begin{matrix}
  a_{1n} & b_{1n}  \\
  -b_{1n} &   a_{1n}
  \end{matrix}  
  \\
\hline

  \bigvd
  & \rvline & \bigvd  & \rvline &
  \bigvd
  \\
\hline
    \begin{matrix}
  a_{n1} & b_{n1}  \\
  -b_{n1} &   a_{n1}
  \end{matrix}
  & \rvline & \bigld  & \rvline &
  \begin{matrix}
  a_{nn} & b_{nn}  \\
  -b_{nn} &   a_{nn}
  \end{matrix}  
\end{pmatrix}
\]
\end{itemize}

Consider the \cC linear map $\cC^n\to \cC^n$ given by $z\to zi$. This is $R_A$ where 

\[A= \begin{pmatrix}
  i & \ldots &   0\\
  \vdots & \ddots & \vdots \\
   0 & \ldots &   i\\
\end{pmatrix} =iI\]

For this matrix we have 
\[\rho_n(A)=\begin{pmatrix}
  \begin{matrix}
  0 & 1  \\
  -1 &   0
  \end{matrix}
  & \rvline & \bigld  & \rvline &
  \bigzero
  \\
\hline

  \bigvd
  & \rvline & \bigvd  & \rvline &
  \bigvd
  \\
\hline
    \bigzero
  & \rvline & \bigld  & \rvline &
  \begin{matrix}
  0 & 1  \\
  -1 &   0
  \end{matrix}  
\end{pmatrix}=\cin
\]

A map $f:\cC^n\to \cC^n$ is \cC linear if it is real linear and $f(zi)=f(z)i$.

Let $bar{f}:\rR^{2n}\to \rR^{2n}$ be the corresponding \rR linear map and suppose this has matrix $B\in \mr{2n}$. Then the complex linearity requirement is $R_B\circ R_{\cin}=R_{\cin}R_{B}$. 

Since $R_X=R_Y \iff X=Y$ we see this is equivalent to asking $B\cin=\cin B$. i.e. $B\in \mr{2n}$ corresponds under $\theta_n$ to a complex linear map $\iff$  $B\cin=\cin B$.

We'd proved

\begin{cor}
The image of $\rho_n:\mnc \to \mr{2n}$ is the set of all of all matrices in \mr{2n} which commute with \cin .
\end{cor}

Remark: This shows that $\rho_n$ is not surjective.

\begin{lemma}
There is an injective group homomorphism $\rho_n : \glnc \to \glr{2n}$, given by restricting $\rho_n : \mnc \to \mr{2n}$.
\end{lemma}

\begin{proof}
We just have to check that if $A\in \glnc$, then $\rho_n(A)$ is invertible. Clearly $\rho_n(AA^{-1})=\rho_n(A^{-1}A)=\rho_n(I_n)$ so by 1.1.10. $\rho_n(A)\rho_n(A^{-1})=\rho_n(A^{-1})\rho_n(A)=\rho_n(I_n)=I_{2n}$. 

$\therefore \rho_n(A^{-1})=\rho_n(A)^{-1}$, hence $\rho_n(A)\in\glr{2n}$. So $\rho_n:\glnc\to\glr{2n}$, and by 1.1.10 this is a (multiplicative) group homomorphism 
\end{proof}

Now for quaternion matrices. 

First observe that there is a \cC linear isomorphism $\phi_n: \hH^n \to \cC^{2n}$ given by $\phi_n(z_1+w_1 j, \ldots, z_n+w_n j)=(z_1,w_1,\ldots,z_n, w_n).$

(exercise to figure out $a+bi+cj+dk$ as  $z+wj$, with $\rR\subset \cC \subset \hH$.)

\begin{prop}
There is an injective \cC linear map $\psi_n:\mnh \to \mc{2n}$ s.t. the following square commutes:

\begin{center}
\begin{tikzcd}
  \hH^n \arrow{d}{R_A} \arrow{r}{\phi_n}
    & \cC^{2n} \arrow{d}{R_{\psi_n(A)}} \\
  \hH^n \arrow{r}{\phi_n}
&\cC^{2n} \end{tikzcd}
\end{center}

i.e. $\phi_n\circ R_A = R_{\psi_n(A)}\circ \phi_n$. Moreover, $\psi_n$ satisfies $\psi_n(AB)=\psi_n(A)\psi_n(B)$.
\end{prop}

\begin{proof}
Analogous to that of prop 1.1.8 and lemma 1.1.10. Exercise! 
\end{proof}

Remark: It is easily checked (exercise!) that $\psi_1(z+wj) = \begin{pmatrix} z &w \\ -\bar{w}& \bar{z}\end{pmatrix}$

More generally, image of $\psi_n$ consists of block matrices with blocks of this form (analogous to $\rho_n$).

By restricting to invertible matrices we obtain:

\begin{cor}
There is an injective group homomorphism $\psi_n:\glnh \to \glc{2n}$.
\end{cor}
\begin{proof}
Analogous to 1.1.12 - exercise.
\end{proof}

( you can compose the maps then to get a real $4n$ matrix from a quaternionic one)

Composing $\rho_{2n}$ and $\psi_n$ gives

 \begin{cor}
There is an injective \rR linear map resp. group homomorphism given by $\rho_{2n}\circ \psi_n:\mnh \to \mr{4n}$ resp. $\rho_{2n}\circ \psi_n:\glnh \to \glr{4n}$.
\end{cor}

Slogan: all groups of \hH or \cC matrices can be viewed as groups of real matrices!

\begin{defn}
(1.1.16) For $A\in \mnh$, $\det(A):=\det \psi_n(A)$.
\end{defn}

\section{Lecture 5 06/10/23}

(Talking about matrices and linear maps, and in \rR , \cC and \hH, there's a standard basis given to go between linear maps and matrices.

\begin{center}
\begin{tikzcd}
  \cC^n \arrow{d} \arrow{r}
    & \cC^{n} \arrow{d} \\
  \rR^{2n} \arrow{r} 
&\rR^{2n} 
\end{tikzcd}
\end{center}
and you can go between \rR and \cC with a canonical map, where you forget the complex structure going to \rR from \cC or by pairing up the pairs of reals going to \cC .)

Last time:

\begin{itemize}
\item There is a canonical \cC linear isomorphism $\phi_n:\hH^n\to \cC^{2n}$ given by $\phi_n(z_1+w_1 j, \ldots, z_n+w_n j)=(z_1,w_1,\ldots,z_n, w_n).$
\item There is an injective homomorphism of complex algebras $\psi_n: \mnh \to \mc{2n}$ such that the following diagram commutes
\begin{center}
\begin{tikzcd}
  \hH^n \arrow{d}{R_A} \arrow{r}{\phi_n}
    & \cC^{2n} \arrow{d}{R_{\psi_n(A)}} \\
  \hH^n \arrow{r}{\phi_n}
&\cC^{2n} 
\end{tikzcd}
\end{center}
\item If $A\in \mnh$ then $\det(A):=\det \psi_n(A)$.
\end{itemize}

\begin{prop}
(1.1.17 - need to fix the numbering) For $A\in \mnh$, $A$ is invertible $\iff \det A \neq 0$, i.e. $\glnh=\{A\in \mnh | \det A \neq 0\}$.
\end{prop}

\begin{proof}
We claim that $A$ is invertible $\iff$ $\psi_n(A)$ is invertible.

(\ra) This is immediate from the multiplicative properties of $\psi_n$ in 1.1.13.

(\la) In 1.1.14 we noted that the restricted map $\psi_n:\glnh\to\glc{2n}$ is a group homomorphism. $\therefore$ if $\psi_n(A)\in\glc{2n}$ (i.e. is invertible) for $A\in \mnh$, then since $\im \psi_n$ is a subgroup of $\glc{2n}$, $\exists B\in \glnh$ s.t.  $\psi_n(B)=[\psi_n(A)]^{-1}$. (Want to show now that $B$ is the inverse of $A$.)

We have $\psi_n(AB)=\psi_n(A)\psi_n(B)=\psi_n(A)[\psi_n(A)]^{-1}=I_{2n}$. But $\psi_n$ is injective, so we must have $AB=I_n$. Similarly $BA=I$. $\therefore B=A^{-1}$ i.e. $A$ is invertible.

So the claim is true. 
\begin{gather*}
\therefore A \text{ is invertible } \underset{\text{claim}}{\iff} \psi_n \text{ is invertible } \\ 
\underset{\text{elementary linear algebra}}{\iff} \det \psi_n(A)\neq 0 \underset{\text{defn 1.1.16}}{\iff} \det A \neq 0 
\end{gather*}
\end{proof}

\subsubsection{Orthogonal Groups}

From now on we will only consider (skew) fields \rR, \cC, \hH (=\fF). 

Recall that an inner product on a vector space $V$ over \fF is a map $\ipm{\,\,}{}:V\times V \to\fF$ which is bilinear (i.e. linear in each entry) and is positive definite, i.e. $\ipm{v}{v} \geq 0\; \forall v\in V$. If $v\neq 0, \ipm{v}{v}>0$. e.g. dot product in $\rR^n$.

\begin{defn}
\phantom{x}\\
\begin{itemize}
\item[a] The \underline{standard inner product} in $\fF^n$ is given by $\ipm{(x_1,\ldots,x_n)}{(y_1,\ldots,y_n)}$ $= x_1\bar{y_1}+\ldots + x_n\bar{y_n}$, where $\bar{y_i}$ is the conjugate of $y_i$. (If $y_i\in\rR$ then $\bar{y_i}=y_i$). This is the dot product if $\fF=\rR$, and is called a \underline{Hermitian} inner product if $\fF=\cC$ or \hH.
\item[b] The standard basis for $\fF^n$ is $(1,0,\ldots,0)$, $(0,1,\ldots,0)$, $\ldots $, $(0,0,\ldots,1)$.
\end{itemize}
\end{defn}

Remarks: As $x\bar{x}=\abs{x}^2$ for all $x\in \fF^n$ we see that $\ipm{x}{x}\geq 0  \forall x \in \fF^n$ for the standard inner products.

For $\lambda \in \fF$,

\begin{itemize}
\item $\ipm{\lambda x}{ y}=\lambda \ipm{x}{y}$
\item $\ipm{ x}{ \lambda y}=\ipm{x}{y}\bar{\lambda}$ (NB for $q_1,q_2\in \hH$ $\bar{q_1q_2}=\bar{q_2}\bar{q_1}$.)
\item $\ol{\ipm{ x}{y}}=\ipm{ y}{x}$.
\end{itemize}

In the real case, vectors $x,y$ are orthogonal if $\ipm{ x}{y}=0$. A basis $\{v_1,\ldots,v_n\}$ for $\rR^n$ is orthonormal if $\abs{v_i}=1 \forall i$ and $\ipm{ v_i}{v_j}=0$ for $i\neq j$.

Exactly the same language is used if $\fF = \cC, \hH$.

\begin{lemma}
$\{v_1,\ldots,v_n\} \in \cC^n$ is a (Hermitian) orthonormal basis $\iff \{\theta_n(v_1), \theta_n(iv_1), \ldots , \theta_n(v_n), \theta_n(iv_n)\}$ is an orthonormal basis for $\rR^n$. 

($\theta_n:\cC^n \cong \rR^{2n}$).
\end{lemma}

\begin{proof}
An easy computation sows that 

\[\underbrace{\ipm{x}{y}_\cC}_{\text{Hermitian I.P. on } \cC^n} = \underbrace{\ipm{\theta_n(x)}{\theta_n(y)}_\rR}_{\text{dot product on } \rR^n}+i\ipm{\theta_n(x)}{\theta_n(iy)}_\rR\].

Thus $\ipm{x}{y}=0 \iff  \ipm{\theta_n(x)}{\theta_n(y)}_\rR $ and $\ipm{\theta_n(x)}{\theta_n(iy)}_\rR $. The result now follows easily. (Exercise: complete the argument)
\end{proof}
\end{document}
